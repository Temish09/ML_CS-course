{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_CS_work.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRshuJDf9iVUdw8fkezKE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Temish09/ML_CS-course/blob/main/ML_CS_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Классификация новостей\n"
      ],
      "metadata": {
        "id": "9mj5jolJvZ8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работа Биджиева Темирлана, группа 519/2.\n",
        "\n",
        "Спецкурс: \"Машинное обучение и искусственный интеллект\".\n",
        "\n",
        "Преподаватель: Смирнов Илья Николаевич."
      ],
      "metadata": {
        "id": "G7hsdfSyvnqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Kde1LdCwrZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача классификации новостных текстов по тематикам."
      ],
      "metadata": {
        "id": "DF9NFjJk8elD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Подгрузим данные новостей 20 новостных групп."
      ],
      "metadata": {
        "id": "pzIdfaiXvlSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "for topic in newsgroups_train.target_names:\n",
        "  print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJb3bkTVvkLS",
        "outputId": "52c9c940-e493-48b3-82c5-e27ba3ac2314"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "alt.atheism\n",
            "comp.graphics\n",
            "comp.os.ms-windows.misc\n",
            "comp.sys.ibm.pc.hardware\n",
            "comp.sys.mac.hardware\n",
            "comp.windows.x\n",
            "misc.forsale\n",
            "rec.autos\n",
            "rec.motorcycles\n",
            "rec.sport.baseball\n",
            "rec.sport.hockey\n",
            "sci.crypt\n",
            "sci.electronics\n",
            "sci.med\n",
            "sci.space\n",
            "soc.religion.christian\n",
            "talk.politics.guns\n",
            "talk.politics.mideast\n",
            "talk.politics.misc\n",
            "talk.religion.misc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Произведем препроцессинг текста: \n",
        "  1. Удалим слова, не несущие смысла;\n",
        "  2. Удалим знаки препинания;\n",
        "  3. Все заглавные буквы заменим на строчные;\n",
        "  4. Токенизируем текст; "
      ],
      "metadata": {
        "id": "JhpMvJgExR9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re\n",
        "import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ILpVdXw_81",
        "outputId": "55d2e283-0364-4602-9117-c6c6461f206b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(texts):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  regex = re.compile('[^a-z A-Z]')\n",
        "  preprocess_texts = []\n",
        "\n",
        "  for i in tqdm.tqdm(range(len(texts))):\n",
        "    text = texts[i].lower()\n",
        "    text = regex.sub(' ', text)\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    preprocess_texts.append(' '.join(filtered_sentence))\n",
        "\n",
        "  return preprocess_texts\n"
      ],
      "metadata": {
        "id": "LkLtUdutyGKK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups_train.keys())\n",
        "newsgroups_train['preprocess_data'] = preprocess_text(newsgroups_train.data)\n",
        "print('\\n', newsgroups_train.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXYULTCfy6EI",
        "outputId": "7d55f387-eb2e-4cc4-9b30-8a3491cb5249"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11314/11314 [00:16<00:00, 703.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'preprocess_data'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups_test.keys())\n",
        "newsgroups_test['preprocess_data'] = preprocess_text(newsgroups_test.data)\n",
        "print('\\n', newsgroups_test.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL76ch92zc15",
        "outputId": "c2b2cf8b-662b-45ac-c24c-5fe1ac5e4df2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7532/7532 [00:10<00:00, 729.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'preprocess_data'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Применим стемминг, чтобы выделять корни слов, тем самым слова с разным окончанием, будут восприниматься как одно слово."
      ],
      "metadata": {
        "id": "DjRltUmnz64E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer"
      ],
      "metadata": {
        "id": "YcOMx69U0OfC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming_texts(texts):\n",
        "  st = LancasterStemmer()\n",
        "  stem_text = []\n",
        "  for text in tqdm.tqdm(texts):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stem_text.append(' '.join([st.stem(word) for word in word_tokens]))\n",
        "  \n",
        "  return stem_text"
      ],
      "metadata": {
        "id": "fS94fyB40XQs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train['data_stemming'] = stemming_texts(newsgroups_train.preprocess_data)\n",
        "newsgroups_test['data_stemming'] = stemming_texts(newsgroups_test.preprocess_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbw5UX8O0zeW",
        "outputId": "40085be4-b150-4813-8700-5d805e94fa70"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11314/11314 [00:44<00:00, 253.71it/s]\n",
            "100%|██████████| 7532/7532 [00:27<00:00, 271.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Будем по-разному векторизировать наши данные и сравним качество SVM модели при каждом из способов векторизации."
      ],
      "metadata": {
        "id": "5GMa0AkA4D0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bow(vectorizer, train, test):     # Bag Of Words\n",
        "  train_bow = vectorizer.fit_transform(train)\n",
        "  test_bow = vectorizer.transform(test)\n",
        "\n",
        "  return train_bow, test_bow"
      ],
      "metadata": {
        "id": "fNkxVJfG4pib"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применим векторизацию через CountVectorizer."
      ],
      "metadata": {
        "id": "qiyDl1oe4bo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X_train_bow_stem, X_test_bow_stem = bow(CountVectorizer(), newsgroups_train.data_stemming, newsgroups_test.data_stemming)"
      ],
      "metadata": {
        "id": "xUXPW--44WmV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применим векторизация через TF x IDF"
      ],
      "metadata": {
        "id": "dVz8AMER6Dtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X_train_tfdidf_stem, X_test_tfidf_stem = bow(TfidfVectorizer(), newsgroups_train.data_stemming, newsgroups_test.data_stemming)"
      ],
      "metadata": {
        "id": "AW9XeN_55--X"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применим векторизацию через TF x IDF с ngram. "
      ],
      "metadata": {
        "id": "dChDja5y6uoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ngram_stem, X_test_ngram_stem = bow(TfidfVectorizer(ngram_range=(1,2)), newsgroups_train.data_stemming, newsgroups_test.data_stemming)"
      ],
      "metadata": {
        "id": "gO3sTEHi6sIf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Сравним способы препроцессинга через качество работы SVM классификатора."
      ],
      "metadata": {
        "id": "wx4GAIVM7Gh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели"
      ],
      "metadata": {
        "id": "8JO6FxeFCOVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# CountVectorizer\n",
        "lsvc_count = LinearSVC()\n",
        "lsvc_count.fit(X_train_bow_stem, newsgroups_train.target)\n",
        "lsvc_count_test_pred = lsvc_count.predict(X_test_bow_stem)\n",
        "\n",
        "# TFIDF\n",
        "lsvc_tfidf = LinearSVC()\n",
        "lsvc_tfidf.fit(X_train_tfdidf_stem, newsgroups_train.target)\n",
        "lsvc_tfidf_test_pred = lsvc_tfidf.predict(X_test_tfidf_stem)\n",
        "\n",
        "# NGRAM\n",
        "lsvc_ngram = LinearSVC()\n",
        "lsvc_ngram.fit(X_train_ngram_stem, newsgroups_train.target)\n",
        "lsvc_ngram_test_pred = lsvc_ngram.predict(X_test_ngram_stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hBWEvV17Arm",
        "outputId": "6fb10546-5c90-4690-b060-242cc925da2e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитам несколько метрик для каждой модели:\n",
        "\n",
        "1. Accuracy\n",
        "2. Precision\n",
        "3. Recall\n",
        "4. F1"
      ],
      "metadata": {
        "id": "sOentFeyCQpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")"
      ],
      "metadata": {
        "id": "uHWClp3BAs7k"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy\n",
        "\n",
        "Видно, что у препроцессинга tf_idf с ngram точность самая большая."
      ],
      "metadata": {
        "id": "IrTnyqS-CvZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsvc_count_acc = accuracy_score(lsvc_count_test_pred, newsgroups_test.target)\n",
        "print(lsvc_count_acc)\n",
        "\n",
        "lsvc_tfidf_acc = accuracy_score(lsvc_tfidf_test_pred, newsgroups_test.target)\n",
        "print(lsvc_tfidf_acc)\n",
        "\n",
        "lsvc_ngram_acc = accuracy_score(lsvc_ngram_test_pred, newsgroups_test.target)\n",
        "print(lsvc_ngram_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72vVSJmcCprX",
        "outputId": "44fb0949-e9b4-4df4-b46c-0f66e8ae15ef"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.781465746149761\n",
            "0.8437334041423261\n",
            "0.8555496548061604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision\n",
        "\n",
        "Все также, у препроцессинга tf_idf с ngram точность самая большая."
      ],
      "metadata": {
        "id": "38alvTgZIJO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsvc_count_prec = precision_score(lsvc_count_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_count_prec)\n",
        "\n",
        "lsvc_tfidf_prec = precision_score(lsvc_tfidf_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_tfidf_prec)\n",
        "\n",
        "lsvc_ngram_prec = precision_score(lsvc_ngram_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_ngram_prec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTOe0Vi6IJ3V",
        "outputId": "cce02023-388c-4058-ea52-a6e7bdda4d89"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7860258338011773\n",
            "0.849480236165773\n",
            "0.8620500765154635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall\n",
        "\n",
        "Результаты аналогичные."
      ],
      "metadata": {
        "id": "FCBWZgaLIKYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsvc_count_recall = recall_score(lsvc_count_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_count_recall)\n",
        "\n",
        "lsvc_tfidf_recall = recall_score(lsvc_tfidf_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_tfidf_recall)\n",
        "\n",
        "lsvc_ngram_recall = recall_score(lsvc_ngram_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_ngram_recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvZ8QITbIKmY",
        "outputId": "87f6b2a1-fbfc-43d8-fbe9-29a6a5fa4501"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.781465746149761\n",
            "0.8437334041423261\n",
            "0.8555496548061604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score\n",
        "\n",
        "Результаты аналогичные."
      ],
      "metadata": {
        "id": "Ho45j3fNDbK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsvc_count_f1 = f1_score(lsvc_count_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_count_f1)\n",
        "\n",
        "lsvc_tfidf_f1 = f1_score(lsvc_tfidf_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_tfidf_f1)\n",
        "\n",
        "lsvc_ngram_f1 = f1_score(lsvc_ngram_test_pred, newsgroups_test.target, average='weighted')\n",
        "print(lsvc_ngram_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh5ZPGe1DVgP",
        "outputId": "2e08d2d6-4f05-483e-dd9e-c228a39ea7c6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7823339419720996\n",
            "0.8448702531127927\n",
            "0.8569422775357723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод: из трех методов препроцессинга, наибольшую ценность показал метод tf_idf с ngram."
      ],
      "metadata": {
        "id": "vlZnvU4fJIM-"
      }
    }
  ]
}